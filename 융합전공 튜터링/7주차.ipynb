{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 웹 크롤링\n",
    "\n",
    "#### requests\n",
    "\n",
    "백그라운드에서 웹 크롤링을 진행.  \n",
    "\n",
    "    requests.get(\"url\")\n",
    "    \n",
    "입력한 URL을 호출함.\n",
    "\n",
    "**<HTTP 응답 번호>**\n",
    "- 200은 성공(205: ???)  \n",
    "- 300번대는 리다이렉션 메시지  \n",
    "- 400번대는 오류 메시지(403: 권한 없음, 404: 존재하지 않는 페이지)\n",
    "\n",
    "웹페이지 개발자 도구 이용  \n",
    "보통은 개발자 도구 페이지의 개체를 마우스 우클릭하면 나오는 XPath를 사용하여 코드 작성.\n",
    "\n",
    "#### bs4\n",
    "\n",
    "con.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib as ulb\n",
    "import requests as rqs\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "url = \"https://ko.wikipedia.org/wiki/조선_세종\"\n",
    "\n",
    "a = rqs.get(\"http://www.naver.com\")\n",
    "b = bs(a.content, \"lxml\")\n",
    "\n",
    "lst_kw = b.find_all(name = \"a\", class_ = \"ah_k\", attr = {\"data-clk\" : \"lve.keyword\"})\n",
    "b\n",
    "# lst_rk = b.fine_all(name = \"span\", class_ = \"ah_r\")\n",
    "\n",
    "# for line in :\n",
    "    # print(line.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이썬으로 매일 오전 8시, 오후 8시 실시간 검색어를 7일간 크롤링하기.  \n",
    "검색어의 순위 정보도 크롤링돼야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib as ulb\n",
    "import requests as rqs\n",
    "from bs4 import BeautifulSoup as soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://datalab.naver.com/keyword/realtimeList.naver?datetime=2019-05-27T08%3A00%3A00&where=main\n",
      "https://datalab.naver.com/keyword/realtimeList.naver?datetime=2019-05-27T08%3A00%3A00&where=main\n"
     ]
    }
   ],
   "source": [
    "date = [\"2019-05-16\", \"2019-05-17\", \"2019-05-18\", \"2019-05-19\", \"2019-05-20\", \"2019-05-21\", \"2019-05-22\", \"2019-05-23\", \"2019-05-27\"]\n",
    "time = [\"08%3A00%3A00\", \"20%3A00%3A00\"]\n",
    "\n",
    "url = \"https://datalab.naver.com/keyword/realtimeList.naver?datetime=%sT%s&where=main\" %(date[-1], time[0])\n",
    "print(url)\n",
    "\n",
    "rq_header = {\n",
    "    \"User-Agent\" : (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.134 Safari/537.36 Vivaldi/2.5.1525.40\")\n",
    "}\n",
    "\n",
    "page = rqs.get(url, headers = rq_header)\n",
    "page.encoding = \"utf-8\"\n",
    "print(page.url)\n",
    "# print(page.history)\n",
    "\n",
    "source_page = soup(page.content, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_sql(d, t, k, r, c, s):\n",
    "    dict_age = {\"all\" : \"전체\", \"10s\" : \"10대\", \"20s\" : \"20대\", \"30s\" : \"30대\", \"40s\" : \"40대\", \"50s\" : \"50대 이상\"}\n",
    "    \n",
    "    dict_time = {\"08%3A00%3A00\": \"08:00\", \"20%3A00%3A00\": \"20:00\"}\n",
    "    \n",
    "    sql = \"\"\"INSERT INTO Search_Keywords(Date, Time, Keyword, Rank, Category, Source)\n",
    "VALUES ('%s', '%s', '%s', '%s', '%s', '%s');\"\"\" %(d, dict_time[t], k, r, dict_age[c], s)\n",
    "        \n",
    "    return sql\n",
    "    \n",
    "    \n",
    "def crawler(dates, times):\n",
    "    rq_header = {\n",
    "    \"User-Agent\" : (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.134 Safari/537.36 Vivaldi/2.5.1525.40\")\n",
    "    }\n",
    "    \n",
    "    ages = [\"all\", \"10s\", \"20s\", \"30s\", \"40s\", \"50s\"]\n",
    "    \n",
    "    for d in dates:  # 일자별로 키워드 수집\n",
    "        for t in times:  # 시간별로 키워드 수집\n",
    "            url = \"https://datalab.naver.com/keyword/realtimeList.naver?datetime=%sT%s&where=main\" %(d, t)\n",
    "            \n",
    "            page = rqs.get(url, headers = rq_header)\n",
    "            page.encoding = \"utf-8\"\n",
    "            \n",
    "            source_page = soup(page.content, \"lxml\")\n",
    "            \n",
    "            for i in range(len(ages)):  # 나이별로 키워드 수집\n",
    "                keywords = source_page.select(\".section_lst_area.carousel_area.lst_narrow .keyword_rank div[data-age='%s'] span\" %ages[i])\n",
    "                \n",
    "                kwds_txt = []\n",
    "                for j in range(len(keywords)):  # 키워드(텍스트)만 추출하여 txt에 추가\n",
    "                    kwds_txt.append(keywords[j].text)\n",
    "                \n",
    "                for j in range(len(kwds_txt)):\n",
    "                    sql = export_to_sql(d, t, kwds_txt[j], j + 1, ages[i], \"Naver\")  # Date, Time, Keyword, Rank, Category, Source\n",
    "                    # cursor.execute(sql)\n",
    "    \n",
    "    print(\"end\")\n",
    "\n",
    "    # cnx.commit()\n",
    "    print(\"커밋됨.\")\n",
    "\n",
    "    # cnx.close()\n",
    "    print(\"종료됨.\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n",
      "커밋됨.\n",
      "종료됨.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates = [\"2019-05-16\", \"2019-05-17\", \"2019-05-18\", \"2019-05-19\", \"2019-05-20\", \"2019-05-21\", \"2019-05-22\", \"2019-05-23\"]\n",
    "\n",
    "times = [\"08%3A00%3A00\", \"20%3A00%3A00\"]\n",
    "\n",
    "crawler([\"2019-05-23\"], [\"08%3A00%3A00\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['효린',\n",
       " '구하라',\n",
       " '위메프복권',\n",
       " '효린 학폭',\n",
       " '아스달 연대기',\n",
       " '효린 일진',\n",
       " '제주공항',\n",
       " '효린 카톡',\n",
       " '비트코인',\n",
       " '맨유 뮌헨',\n",
       " '봉준호 감독',\n",
       " '창원 날씨',\n",
       " '일기예보',\n",
       " '기온별 옷차림',\n",
       " '구해줘 홈즈',\n",
       " '티몬데이',\n",
       " '인천날씨',\n",
       " '날씨예보',\n",
       " '영화 기생충 줄거리',\n",
       " '김해날씨']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keywords = source_page.select(\".section_lst_area.carousel_area.lst_narrow .keyword_rank > .rank_inner *\")\n",
    "# rank = source_page\n",
    "keywords_all = source_page.select(\".section_lst_area.carousel_area.lst_narrow .keyword_rank div[data-age='all'] span\")\n",
    "\n",
    "res_all = []\n",
    "for i in range(len(keywords_all)):\n",
    "    res_all.append(keywords_all[i].text)\n",
    "\n",
    "res_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['맨유 뮌헨',\n",
       " '구하라',\n",
       " '효린',\n",
       " '버스도착정보',\n",
       " '태풍',\n",
       " '에이틴',\n",
       " '2019 6월 모의고사',\n",
       " '맨유 레전드 매치',\n",
       " '기온별 옷차림',\n",
       " '빅히트샵',\n",
       " '진격의 거인 3기 17화',\n",
       " '김해날씨',\n",
       " '군산날씨',\n",
       " '인천날씨',\n",
       " '안산 날씨',\n",
       " '목포날씨',\n",
       " '익산 날씨',\n",
       " '원주 날씨',\n",
       " '순천날씨',\n",
       " '전주날씨']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_10s = source_page.select(\".section_lst_area.carousel_area.lst_narrow .keyword_rank div[data-age='10s'] span\")\n",
    "\n",
    "res_10s = []\n",
    "for i in range(len(keywords_10s)):\n",
    "    res_10s.append(keywords_10s[i].text)\n",
    "\n",
    "res_10s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['구하라',\n",
       " '효린',\n",
       " '기온별 옷차림',\n",
       " '효린 카톡',\n",
       " '창원 날씨',\n",
       " '인천날씨',\n",
       " '위메프복권',\n",
       " '대탈출 시즌2',\n",
       " '아스달 연대기',\n",
       " '천안날씨',\n",
       " '수원날씨',\n",
       " '서울 날씨',\n",
       " '춘천 날씨',\n",
       " '부산 날씨',\n",
       " '청주 날씨',\n",
       " '안산 날씨',\n",
       " '제주공항',\n",
       " '김해날씨',\n",
       " '평택 날씨',\n",
       " '대전 날씨']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_20s = source_page.select(\".section_lst_area.carousel_area.lst_narrow .keyword_rank div[data-age='20s'] span\")\n",
    "\n",
    "res_20s = []\n",
    "for i in range(len(keywords_20s)):\n",
    "    res_20s.append(keywords_20s[i].text)\n",
    "\n",
    "res_20s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['효린',\n",
       " '구하라',\n",
       " '위메프복권',\n",
       " '효린 카톡',\n",
       " '효린 일진',\n",
       " '아스달 연대기',\n",
       " '효린 학폭',\n",
       " '제주공항',\n",
       " '비트코인',\n",
       " '티몬데이',\n",
       " '구해줘 홈즈',\n",
       " '일기예보',\n",
       " '채지안',\n",
       " '네이버날씨',\n",
       " '신의탑',\n",
       " '이번주날씨',\n",
       " '제주날씨',\n",
       " '황금종려상',\n",
       " '기상청',\n",
       " '창원 날씨']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_30s = source_page.select(\".section_lst_area.carousel_area.lst_narrow .keyword_rank div[data-age='30s'] span\")\n",
    "\n",
    "res_30s = []\n",
    "for i in range(len(keywords_30s)):\n",
    "    res_30s.append(keywords_30s[i].text)\n",
    "\n",
    "res_30s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['효린',\n",
       " '구하라',\n",
       " '위메프복권',\n",
       " '효린 학폭',\n",
       " '효린 일진',\n",
       " '아스달 연대기',\n",
       " '제주공항',\n",
       " '봉준호 감독',\n",
       " '효린 카톡',\n",
       " '영화 기생충 줄거리',\n",
       " '날씨예보',\n",
       " '구해줘 홈즈',\n",
       " '티몬데이',\n",
       " '비트코인',\n",
       " '기생충 개봉일',\n",
       " '씨스타',\n",
       " '케빈나',\n",
       " '로또860회당첨번호',\n",
       " '일기예보',\n",
       " '기상청']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_40s = source_page.select(\".section_lst_area.carousel_area.lst_narrow .keyword_rank div[data-age='40s'] span\")\n",
    "\n",
    "res_40s = []\n",
    "for i in range(len(keywords_40s)):\n",
    "    res_40s.append(keywords_40s[i].text)\n",
    "\n",
    "res_40s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['구하라',\n",
       " '효린',\n",
       " '케빈나',\n",
       " '봉준호 감독',\n",
       " '효린 학폭',\n",
       " '영화 기생충 줄거리',\n",
       " '조진래',\n",
       " '송강호',\n",
       " '생일도',\n",
       " '송인택',\n",
       " '제주공항',\n",
       " '기생충 개봉일',\n",
       " '효린 일진',\n",
       " '나상욱',\n",
       " '로또860회당첨번호',\n",
       " '청년 전세자금대출',\n",
       " '날씨예보',\n",
       " 'lpga실시간스코어',\n",
       " '파킹통장',\n",
       " '아스달 연대기']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_50s = source_page.select(\".section_lst_area.carousel_area.lst_narrow .keyword_rank div[data-age='50s'] span\")\n",
    "\n",
    "res_50s = []\n",
    "for i in range(len(keywords_50s)):\n",
    "    res_50s.append(keywords_50s[i].text)\n",
    "\n",
    "res_50s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql as pms\n",
    "\n",
    "cnx = pms.connect(host = \"plen-tutoring.mysql.database.azure.com\", \n",
    "                  port = 3306, \n",
    "                  user = \"plentir@plen-tutoring\", \n",
    "                  password = \"yjy990810!\", \n",
    "                  database = \"web_crawling\", \n",
    "                  charset = \"utf8\")\n",
    "\n",
    "cursor = cnx.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mk_table = \"\"\"CREATE TABLE Search_Keywords(\n",
    "ID INT UNSIGNED NOT NULL AUTO_INCREMENT, \n",
    "Date VARCHAR(10) NOT NULL, \n",
    "Time VARCHAR(8) NOT NULL, \n",
    "Keyword VARCHAR(255) NOT NULL, \n",
    "Rank INT NOT NULL, \n",
    "Category VARCHAR(20) NOT NULL, \n",
    "Source VARCHAR(50) NOT NULL, \n",
    "PRIMARY KEY(ID)\n",
    ") DEFAULT CHARSET = utf8 AUTO_INCREMENT = 1;\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(mk_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
