{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png)\n",
    "![SSU AILAB Logo](http://ailab.ssu.ac.kr/rb/layouts/default/image/common/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Spark Context import 확인\n",
    "(설치 가이드대로 설치했을 경우, sc 라는 변수의 이름으로 SparkContext 가 자동으로 생성되어 있음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_accumulatorServer': <pyspark.accumulators.AccumulatorServer at 0x110525748>,\n",
       " '_batchSize': 0,\n",
       " '_callsite': CallSite(function='<module>', file='/Users/ai/anaconda3/lib/python3.6/site-packages/IPython/utils/py3compat.py', linenum=186),\n",
       " '_conf': <pyspark.conf.SparkConf at 0x1105256d8>,\n",
       " '_encryption_enabled': False,\n",
       " '_javaAccumulator': JavaObject id=o17,\n",
       " '_jsc': JavaObject id=o14,\n",
       " '_pickled_broadcast_vars': <pyspark.broadcast.BroadcastPickleRegistry at 0x11006cb28>,\n",
       " '_python_includes': [],\n",
       " '_temp_dir': '/private/var/folders/_p/rfs0fb9n7b52r9m1tbwndftr0000gn/T/spark-823153ef-c50a-4f3c-94bd-c20665e2a72d/pyspark-20d6e879-947d-497e-8d0b-5156dc44cb95',\n",
       " '_unbatched_serializer': PickleSerializer(),\n",
       " 'appName': 'PySparkShell',\n",
       " 'environment': {'PYTHONHASHSEED': '0'},\n",
       " 'master': 'local[2]',\n",
       " 'profiler_collector': None,\n",
       " 'pythonExec': 'python',\n",
       " 'pythonVer': '3.6',\n",
       " 'serializer': AutoBatchedSerializer(PickleSerializer()),\n",
       " 'sparkHome': None}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) #### 사용자의 Spark Application 이름 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sc.appName = \"TestApp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TestApp'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.appName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### 파일로 부터 Spark RDD 생성\n",
    "(4개의 파티션 생성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "data = sc.textFile(\"sample.txt\", 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### 파티션별 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Once more',\n",
       "  'I summon you',\n",
       "  'Out of the past',\n",
       "  'With poignant love,',\n",
       "  'You who nourished the poet'],\n",
       " ['And the lover.', 'I see your gary eyes', 'Looking out to sea'],\n",
       " ['In those Rockport summers,', 'Keeping a distance', 'Within the closeness'],\n",
       " ['Which was never intrusive', 'Opening out', 'Into the world.']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### 파티션의 개수를 6개로 변경후 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "data1 = data.repartition(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " ['Once more',\n",
       "  'I summon you',\n",
       "  'Out of the past',\n",
       "  'With poignant love,',\n",
       "  'You who nourished the poet'],\n",
       " ['Which was never intrusive', 'Opening out', 'Into the world.'],\n",
       " [],\n",
       " ['In those Rockport summers,', 'Keeping a distance', 'Within the closeness'],\n",
       " ['And the lover.', 'I see your gary eyes', 'Looking out to sea']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(data1.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### xrange, range 차이점 (python 내용)\n",
    "(xrange 는 lazy evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# dataXrange = xrange(1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# type(dataXrange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dataRange = range(1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataRange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# dataXrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(1, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataRange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### 객체로 부터 RDD 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# xrangeRDD = sc.parallelize(dataXrange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "rangeRDD = sc.parallelize(dataRange,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### toDebugString 를 통한 RDD 의 lineage 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# xrangeRDD.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'(8) PythonRDD[10] at RDD at PythonRDD.scala:53 []\\n |  ParallelCollectionRDD[9] at parallelize at PythonRDD.scala:195 []'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rangeRDD.toDebugString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### xrange, range 타입에 따라 생성되는 RDD 의 타입 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# print 'type of RangeRDD: {0}'.format(type(xrangeRDD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of XrangeRDD: <class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    }
   ],
   "source": [
    "print('type of XrangeRDD: {0}'.format(type(rangeRDD)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id : 10\n"
     ]
    }
   ],
   "source": [
    "print('id : {0}'.format(rangeRDD.id()))\n",
    "# print('id : {0}'.format(xrangeRDD.id()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### function 을 정의하고 Spark 의 map 함수 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 데이터 :  [[1], [2], [3], [4], [5], [6], [7], [8, 9]]\n",
      "sub 데이터 :  [[0], [1], [2], [3], [4], [5], [6], [7, 8]]\n"
     ]
    }
   ],
   "source": [
    "def sub(value):\n",
    "    return (value - 1)\n",
    "\n",
    "subRDD = rangeRDD.map(sub)\n",
    "\n",
    "print(\"원본 데이터 : \", rangeRDD.glom().collect())\n",
    "print(\"sub 데이터 : \",subRDD.glom().collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Map 함수와 flatMap 차이점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 'cats'), ('elephant', 'elephants'), ('rat', 'rats'), ('rat', 'rats'), ('cat', 'cats')]\n",
      "['cat', 'cats', 'elephant', 'elephants', 'rat', 'rats', 'rat', 'rats', 'cat', 'cats']\n",
      "5\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']\n",
    "wordsRDD = sc.parallelize(wordsList,4)\n",
    "\n",
    "sigularAndPluralWordsRDDMap = wordsRDD.map(lambda x: (x, x + 's'))\n",
    "sigularAndPluralWordsRDD = wordsRDD.flatMap(lambda x: (x, x + 's'))\n",
    "\n",
    "print(sigularAndPluralWordsRDDMap.collect())\n",
    "print(sigularAndPluralWordsRDD.collect())\n",
    "print(sigularAndPluralWordsRDDMap.count())\n",
    "print(sigularAndPluralWordsRDD.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[range(1, 2), range(1, 3), range(1, 4)]\n"
     ]
    }
   ],
   "source": [
    "simpleRDD = sc.parallelize([2,3,4])\n",
    "print(simpleRDD.map(lambda x: range(1, x)).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [1, 2], [1, 2, 3]]\n",
      "[1, 1, 2, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "simpleRDD = sc.parallelize([2,3,4])\n",
    "print(simpleRDD.map(lambda x: list(range(1, x))).collect())#python 2 결과 - [[1],[1,2],[1,2,3]]\n",
    "print(simpleRDD.flatMap(lambda x: range(1, x)).collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Spark의 filter 함수 적용 과 lambda 사용법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub 데이터 :  [[0], [1], [2], [3], [4], [5], [6], [7, 8]]\n",
      "Partition별 5보다 작은 데이터 [[0], [1], [2], [3], [4], [], [], []]\n",
      "Partition별 5보다 큰 데이터 [[], [], [], [], [], [], [6], [7, 8]]\n",
      "filteredRDD Partition 개수 :  8\n",
      "filteredRDD2 Partition 개수 :  8\n"
     ]
    }
   ],
   "source": [
    "def five(value):\n",
    "    if(value < 5):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "filteredRDD = subRDD.filter(five)\n",
    "filteredRDD2 = subRDD.filter(lambda x: x > 5)\n",
    "\n",
    "print(\"sub 데이터 : \", subRDD.glom().collect())\n",
    "print(\"Partition별 5보다 작은 데이터\", filteredRDD.glom().collect())\n",
    "print(\"Partition별 5보다 큰 데이터\", filteredRDD2.glom().collect())\n",
    "print(\"filteredRDD Partition 개수 : \", filteredRDD.getNumPartitions())\n",
    "print(\"filteredRDD2 Partition 개수 : \", filteredRDD2.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### takeSample 함수를 사용하여 샘플 데이터 추출\n",
    "(withReplacement 는 중복 허용 여부, num 는 샘플 개수, seed 를 주면 항상 같은 샘플 추출)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 3, 4, 4, 0]\n",
      "[0, 4, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "print(filteredRDD.takeSample(withReplacement=True, num=6))\n",
    "print(filteredRDD.takeSample(withReplacement=False, num=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 2, 2, 0, 4]\n",
      "[1, 3, 2, 2, 0, 4]\n"
     ]
    }
   ],
   "source": [
    "print(filteredRDD.takeSample(withReplacement=True, num=6, seed=500))\n",
    "print(filteredRDD.takeSample(withReplacement=True, num=6, seed=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 0, 1, 1, 4]\n",
      "[0, 3, 4, 1, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "print(filteredRDD.takeSample(withReplacement=True, num=6))\n",
    "print(filteredRDD.takeSample(withReplacement=True, num=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### reduceByKey, groupByKey\n",
    "(각각의 return type 숙지, groupByKey 사용법 등 주의)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![reduceByKey() figure](http://spark-mooc.github.io/web-assets/images/reduce_by.png)\n",
    "![groupByKey() figure](http://spark-mooc.github.io/web-assets/images/group_by.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) GroupByKey 에 관한 내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "student :  ParallelCollectionRDD[233] at parallelize at PythonRDD.scala:195\n",
      "choi : <pyspark.resultiterable.ResultIterable object at 0x11062dc18>\n",
      "lee : <pyspark.resultiterable.ResultIterable object at 0x11062d908>\n",
      "kim : <pyspark.resultiterable.ResultIterable object at 0x11062d208>\n",
      "park : <pyspark.resultiterable.ResultIterable object at 0x11062d1d0>\n",
      "\n",
      "\n",
      "choi : ['cs']\n",
      "lee : ['cs']\n",
      "kim : ['ee']\n",
      "park : ['ee', 'biology']\n"
     ]
    }
   ],
   "source": [
    "s = sc.parallelize([('kim', 'ee'), ('choi', 'cs'), \n",
    "                    ('lee', 'cs'), ('park', 'ee'), ('park', 'biology')])\n",
    "sGrouped = s.groupByKey().collect()\n",
    "print(\"student : \", s)\n",
    "for key, value in sGrouped:\n",
    "    print('{0} : {1}'.format(key, value))\n",
    "print('\\n')\n",
    "for key, value in sGrouped:\n",
    "    print('{0} : {1}'.format(key, list(value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "student :  ParallelCollectionRDD[36] at parallelize at PythonRDD.scala:195\n",
      "choi : <pyspark.resultiterable.ResultIterable object at 0x1106122b0>\n",
      "lee : <pyspark.resultiterable.ResultIterable object at 0x110612668>\n",
      "kim : <pyspark.resultiterable.ResultIterable object at 0x110612358>\n",
      "park : <pyspark.resultiterable.ResultIterable object at 0x110612908>\n",
      "\n",
      "\n",
      "choi : ['cs']\n",
      "lee : ['cs']\n",
      "kim : ['ee']\n",
      "park : ['ee']\n"
     ]
    }
   ],
   "source": [
    "#groupByKey 1\n",
    "s = sc.parallelize([('kim', 'ee'), ('choi', 'cs'), ('lee', 'cs'), ('park', 'ee')])\n",
    "sGrouped = s.groupByKey().collect()\n",
    "print(\"student : \", s)\n",
    "for key, value in sGrouped:\n",
    "    print('{0} : {1}'.format(key, value))\n",
    "print('\\n')\n",
    "for key, value in sGrouped:\n",
    "    print('{0} : {1}'.format(key, list(value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am [4, 2]\n",
      "boy [2, 3]\n"
     ]
    }
   ],
   "source": [
    "#groupByKey 2\n",
    "word = sc.parallelize([('am', 4), ('boy', 2), ('am', 2), ('boy', 3)])\n",
    "wordGroup = word.groupByKey()\n",
    "for key, value in wordGroup.collect():\n",
    "    print(key, list(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am 6\n",
      "boy 5\n"
     ]
    }
   ],
   "source": [
    "#groupByKey sum2\n",
    "word = sc.parallelize([('am', 4), ('boy', 2), ('am', 2), ('boy', 3)])\n",
    "wordGroup = word.groupByKey()\n",
    "for key, value in wordGroup.collect():\n",
    "    print(key, sum(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-59-7ddda73dd1e8>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-59-7ddda73dd1e8>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    wc = wordGroup.map(lambda (k, v) : (k, sum(v)))\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "word = sc.parallelize([('am', 4), ('boy', 2), ('am', 2), ('boy', 3)])\n",
    "wordGroup = word.groupByKey()\n",
    "wc = wordGroup.map(lambda (k, v) : (k, sum(v)))\n",
    "for key, value in wc.collect():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am 6\n",
      "boy 5\n"
     ]
    }
   ],
   "source": [
    "#groupByKey map3 \n",
    "word = sc.parallelize([('am', 4), ('boy', 2), ('am', 2), ('boy', 3)])\n",
    "wordGroup = word.groupByKey()\n",
    "wc = wordGroup.map(lambda kv : (kv[0], sum(kv[1])))\n",
    "for key, value in wc.collect():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am 6\n",
      "boy 5\n"
     ]
    }
   ],
   "source": [
    "#groupByKey mapValues3\n",
    "word = sc.parallelize([('am', 4), ('boy', 2), ('am', 2), ('boy', 3)])\n",
    "wordGroup = word.groupByKey()\n",
    "wc = wordGroup.mapValues(sum)\n",
    "for key, value in wc.collect():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am [4, 2]\n",
      "boy [2, 3]\n"
     ]
    }
   ],
   "source": [
    "#groupByKey mapValues4\n",
    "word = sc.parallelize([('am', 4), ('boy', 2), ('am', 2), ('boy', 3)])\n",
    "wordGroup = word.groupByKey()\n",
    "wc = wordGroup.mapValues(list)\n",
    "for key, value in wc.collect():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am [16, 4]\n",
      "boy [4, 9]\n"
     ]
    }
   ],
   "source": [
    "word = sc.parallelize([('am', 4), ('boy', 2), ('am', 2), ('boy', 3)])\n",
    "wordGroup = word.groupByKey()\n",
    "wc = wordGroup.mapValues(lambda x : [i**2 for i in list(x)])\n",
    "for key, value in wc.collect():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) reduceByKey 에 관한 내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am 9\n",
      "boy 14\n"
     ]
    }
   ],
   "source": [
    "#reduceByKey\n",
    "word = sc.parallelize([('am', 4), ('boy', 2), ('am', 2),\n",
    "                       ('am', 3),('boy', 3),('boy', 5),('boy', 4)])\n",
    "wc = word.reduceByKey(lambda a, b : a + b)\n",
    "for key, value in wc.collect():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am 6\n",
      "boy 5\n"
     ]
    }
   ],
   "source": [
    "#reduceBykey\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "word = sc.parallelize([('am', 4), ('boy', 2), ('am', 2), ('boy', 3)])\n",
    "wc = word.reduceByKey(add)\n",
    "for key, value in wc.collect():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'test']\n",
      "<map object at 0x1105b9f60>\n",
      "('this', 1)\n",
      "('is', 1)\n",
      "('a', 1)\n",
      "('test', 1)\n"
     ]
    }
   ],
   "source": [
    "# python map\n",
    "line = \"this is a test\"\n",
    "print(line.split())\n",
    "sp = map(lambda a : (a, 1), line.split())\n",
    "print(sp)\n",
    "for x in sp:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is a test', 'this is the second test', 'this may be the thing']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = sc.textFile(\"short.txt\")\n",
    "text.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'is', 'a', 'test'],\n",
       " ['this', 'is', 'the', 'second', 'test'],\n",
       " ['this', 'may', 'be', 'the', 'thing']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pyspark map\n",
    "text = sc.textFile(\"short.txt\")\n",
    "wc = text.map(lambda line : line.split())\n",
    "wc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'test',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'second',\n",
       " 'test',\n",
       " 'this',\n",
       " 'may',\n",
       " 'be',\n",
       " 'the',\n",
       " 'thing']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pyspark flatMap\n",
    "text = sc.textFile(\"short.txt\")\n",
    "wc = text.flatMap(lambda line : line.split())\n",
    "wc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('this', 1), ('is', 1), ('a', 1), ('test', 1), ('this', 1), ('is', 1), ('the', 1), ('second', 1), ('test', 1), ('this', 1), ('may', 1), ('be', 1), ('the', 1), ('thing', 1)]\n"
     ]
    }
   ],
   "source": [
    "#pyspark flatMap reduceByKey\n",
    "text = sc.textFile(\"short.txt\")\n",
    "wc = text.flatMap(lambda line : line.split()).map(lambda word : (word,1))\n",
    "print(wc.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('this', 3), ('is', 2), ('test', 2), ('may', 1), ('a', 1), ('the', 2), ('second', 1), ('be', 1), ('thing', 1)]\n"
     ]
    }
   ],
   "source": [
    "#pyspark flatMap reduceByKey\n",
    "text = sc.textFile(\"short.txt\")\n",
    "wc = text.flatMap(lambda line : line.split()).map(lambda word : (word,1)) \\\n",
    "        .reduceByKey(lambda c1, c2: c1 + c2)\n",
    "print(wc.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1), ('be', 1), ('is', 2), ('may', 1), ('second', 1), ('test', 2), ('the', 2), ('thing', 1), ('this', 3)]\n",
      "[(3, 'this'), (2, 'is'), (2, 'test'), (2, 'the'), (1, 'may'), (1, 'a'), (1, 'second'), (1, 'be'), (1, 'thing')]\n"
     ]
    }
   ],
   "source": [
    "text = sc.textFile(\"short.txt\")\n",
    "wc = text.flatMap(lambda line : line.split()).map(lambda word : (word,1))\\\n",
    "        .reduceByKey(lambda c1, c2: c1 + c2)\n",
    "    \n",
    "print(wc.sortByKey(ascending=True).collect())\n",
    "print(wc.map(lambda x: (x[1], x[0])).sortByKey(ascending=False).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### RDD의 reduce 함수를 적용(substract 를 사용시 주의)\n",
    "reduce 는 function 이 associative 하고 commutative 해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[p8]data :  [[0], [1], [2], [3], [4], [], [], []]\n",
      "[p8]reduce(sum) :  10\n",
      "[p8]reduce(sub) :  -10\n",
      "[p3]data :  [[], [0, 2], [1, 3, 4]]\n",
      "[p3]reduce(sum) :  10\n",
      "[p3]reduce(sub) :  4\n"
     ]
    }
   ],
   "source": [
    "print(\"[p8]data : \", filteredRDD.glom().collect())\n",
    "# - 는 associative 하지만, commutative하지 못하다.\n",
    "print(\"[p8]reduce(sum) : \", filteredRDD.reduce(lambda a, b: a + b))\n",
    "print(\"[p8]reduce(sub) : \", filteredRDD.reduce(lambda a, b: a - b))\n",
    "\n",
    "# partition에 따라 sub 함수의 값이 달라짐(commutative하지 못하기 때문)\n",
    "rep_filteredRDD = filteredRDD.repartition(3)\n",
    "print(\"[p3]data : \", rep_filteredRDD.glom().collect())\n",
    "\n",
    "# sum 은 partition 을 나눠도 associative 하고 commutative 하기 때문에 결과 같음\n",
    "print(\"[p3]reduce(sum) : \", filteredRDD.repartition(3).reduce(lambda a, b: a + b))\n",
    "print(\"[p3]reduce(sub) : \", filteredRDD.repartition(3).reduce(lambda a, b: a - b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', [1]), ('a', [1, 2])]\n",
      "[('b', 1), ('a', 3)]\n",
      "[('b', 1), ('a', 3)]\n",
      "[('b', 1), ('a', 3)]\n"
     ]
    }
   ],
   "source": [
    "pairRDD = sc.parallelize([('a',1),('a',2),('b',1)])\n",
    "#print(pairRDD.groupByKey().map(lambda (k,v): (k, sum(v))).collect())\n",
    "print(pairRDD.groupByKey().mapValues(lambda x: list(x)).collect())\n",
    "print(pairRDD.groupByKey().map(lambda x: (x[0], sum(x[1]))).collect())\n",
    "print(pairRDD.groupByKey().mapValues(lambda x: sum(x)).collect())\n",
    "print(pairRDD.reduceByKey(add).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ###  WordCount 방법 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduceByKey :  [('i', 2), ('am', 1), ('boy', 1)]\n",
      "groupByKey :  [('i', 2), ('am', 1), ('boy', 1)]\n"
     ]
    }
   ],
   "source": [
    "repetitiveRDD = sc.parallelize([\"i\",\"am\", \"boy\", \"i\"])\n",
    "\n",
    "text1 = repetitiveRDD \\\n",
    ".flatMap(lambda line: line.split()).map(lambda word: (word, 1)) \\\n",
    ".reduceByKey(lambda v1, v2: v1 + v2).collect() # collectAsMap() 사용 시 dictionary 형태로 return\n",
    "\n",
    "print(\"reduceByKey : \", text1)\n",
    "\n",
    "text2 = repetitiveRDD \\\n",
    ".flatMap(lambda line: line.split()).map(lambda word: (word, 1)) \\\n",
    ".groupByKey() \\\n",
    ".map(lambda kv: (kv[0], sum(kv[1]))).collect()\n",
    "print(\"groupByKey : \", text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('am', 1), ('boy', 1), ('i', 2)]\n",
      "[(2, 'i'), (1, 'am'), (1, 'boy')]\n"
     ]
    }
   ],
   "source": [
    "repetitiveRDD = sc.parallelize([\"i\",\"am\", \"boy\", \"i\"])\n",
    "\n",
    "text1 = repetitiveRDD \\\n",
    ".flatMap(lambda line: line.split()).map(lambda word: (word, 1)) \\\n",
    ".reduceByKey(lambda v1, v2: v1 + v2)\n",
    "\n",
    "print(text1.sortByKey(ascending=True).collect())\n",
    "print(text1.map(lambda kv : (kv[1], kv[0])).sortByKey(ascending=False) \\\n",
    "      .collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### RDD의 first 함수는 첫번 째 partition 의 첫번 째 데이터를 Return, take(1)은 first() 와 동일, takeOrdered 는 오름차순 정렬 후 Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Out', 1), ('of', 1), ('poignant', 1), ('poet', 1), ('And', 1), ('lover.', 1), ('eyes', 1), ('out', 2), ('sea', 1), ('Rockport', 1), ('Within', 1), ('Which', 1), ('was', 1)], [('you', 1), ('the', 5), ('With', 1), ('love,', 1), ('You', 1), ('who', 1), ('to', 1), ('Keeping', 1), ('a', 1), ('Opening', 1), ('Into', 1)], [('Once', 1), ('more', 1), ('summon', 1), ('past', 1), ('gary', 1), ('distance', 1), ('closeness', 1), ('never', 1), ('intrusive', 1)], [('I', 2), ('nourished', 1), ('see', 1), ('your', 1), ('Looking', 1), ('In', 1), ('those', 1), ('summers,', 1), ('world.', 1)]]\n",
      "\n",
      "== first ==\n",
      "('Out', 1)\n",
      "\n",
      "== take ==\n",
      "[('Out', 1), ('of', 1), ('poignant', 1), ('poet', 1), ('And', 1), ('lover.', 1), ('eyes', 1), ('out', 2), ('sea', 1), ('Rockport', 1), ('Within', 1), ('Which', 1)]\n",
      "\n",
      "== takeOrdered ==\n",
      "[('And', 1), ('I', 2), ('In', 1)]\n",
      "\n",
      "== top ==\n",
      "based of key :  [('your', 1), ('you', 1), ('world.', 1), ('who', 1), ('was', 1)]\n",
      "based of value :  [('the', 5), ('out', 2), ('I', 2), ('Out', 1), ('of', 1)]\n"
     ]
    }
   ],
   "source": [
    "text = sc.textFile(\"sample.txt\", 4) \\\n",
    ".flatMap(lambda line: line.split()).map(lambda word: (word, 1)) \\\n",
    ".reduceByKey(lambda v1, v2: v1+v2)\n",
    "print(text.glom().collect())\n",
    "print(\"\\n== first ==\")\n",
    "print(text.first()) # First Partition의 첫번째 Element.\n",
    "print(\"\\n== take ==\")\n",
    "print(text.take(12))\n",
    "print(\"\\n== takeOrdered ==\")\n",
    "print(text.takeOrdered(3)) # Collect Action 후에 작은 Element 순으로.\n",
    "print(\"\\n== top ==\")\n",
    " #Collect Action 후에 큰 Element 순으로.\n",
    "print(\"based of key : \", text.top(5))\n",
    "print(\"based of value : \", text.top(5, key = (lambda x : x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
